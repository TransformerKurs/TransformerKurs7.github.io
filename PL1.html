<!DOCTYPE html>

<head>
 <title> Neural networks</title>
</head>

<body>
<h1 id = "Find more">Альтернативные архитектуры НМП</h1>
<h2>Сверточные нейронные сети</h2>
<p>Помимо классических архитектур, функционирующих на основе <b>рекуррентных нейронных сетей</b>, существуют также альтернативные решения. Например, были попытки создания нейросетей машинного перевода на основе <strong>сверточных НС (Сonvolutional neural network, CNN)</strong>. Так, в 2017 году исследователями <em>Facebook AI</em> была выдвинута модель полностью базирующаяся на <b>сверточных нейронных сетях</b>, которая показала результаты высокой эффективности, особенно при обучении работе с задачей <em>sequence-to-sequence</em>. Их модель лучше справляется с <em>обнаружением композиционной структуры в последовательностях</em>, поскольку представления выстроены <em>иерархически</em>. Данная структура также использует <b>гейты и механизм внимания</b>.</p>
<h2>Методы неглубокого обучения</h2>
<p>Обычно для решения задач нейросетевого машинного перевода используются <b>методы глубокого обучения</b>. Но <strong>неглубокие (shallow) НС </strong>также могут быть весьма полезными. <a href="Word2vec.html"><strong>Метод Word2vec</strong></a>, разработанный в 2013 году и являющийся удобным способом для создания <em>векторного представления слов</em> на естественном языке, представляет собой совокупность <em>неглубоких двухслойных моделей на основе нейронных сетей</em>. </p> 
Существует <strong>2 разновидности модели Word2vec</strong>: <b>Skip Gram и Continuous Bag of Words (CBOW)</b>.<br>
Принципы работы <b>word-embedding</b> уже были описаны чуть ранее.
<h2>Архитектура трансформер</h2> 
<p>Существует также интересная разработка, называемая <strong>архитектурой Трансформер (Transformer)</strong>, представленная в 2017 году <em>Google Research и Google Brain</em>. Это новая нейросеть, которая способна производить вычисления <em>параллельно</em>, в отличии от <b>рекуррентных НС</b>, работающих <em>последовательно</em>. Хорошо обученный <b>механизм внимания</b> данной НС ускоряет процесс выполнения поставленной задачи и улучшает долгосрочную память, так как <em>данные передаются от кодера к декодеру напрямую</em>. Архитектура <em>универсальна</em>, потому что способна проводить математические операции с любыми векторами подходящей величины, а, следовательно, может работать с любыми закодированными последовательностями. Помимо механизма внимания архитектура Трансформер приобрела <b>механизм внимания на себя (self-attention)</b>. Как следствие, текущий слой кодера нейросети обращает внимание на свой предшествующий слой, таким образом, каждый следующий слой замечает что-то новое и важное, и НС имеет доступ к любому слову, неважно в каком месте входного текста оно находится. Подводя итог, <strong>архитектура Трансформер</strong> – многообещающая и перспективная разработка, способная справляться даже с весьма комплексными задачами.<img src="https://miro.medium.com/max/1400/1*PXzgmxXEgr9Dn4O8u0NhvQ.png"width="500" alt="Схема Трансформер"></p>

<h1>Список источников:</h1>
<ol> 
<li><a href="#Find more">[Find more]</a>Mingxuan Wang. Neural Machine Translation with Decoding-History Enhanced Attention / Mingxuan Wang, Jun Xie, Zhixing Tan [et al]. // Proceedings of the 27th International Conference on Computational Linguistics. - Santa Fe, New Mexico, USA. - 2018. - PP 1464–1473.
<li>Yonghui Wu. Google’s Neural Machine Translation System: Bridging the Gap between Human and Machine Translation / Yonghui Wu, Mike Schuster, Zhifeng Chen [et al]. – 2016. – PP 1 - 23.</li>
<li>Гусева В.Б. Современные средства машинного перевода // Современные информационные технологии: проблемы и перспективы развития (Екатеринбург, 25 апреля 2017 г.) – Екатеринбург, 2017. – С. 152 - 158</li>
<li>Дьяченко И.Н. Нейронный машинный перевод: преимущества, сложности, перспективы. / Дьяченко И.Н., Матыченко Ю.В. // Языки и литература в поликультурном пространстве. - 2020. – №6. – С. 28 - 33.</li>
<li>Котенко В.В. Перспективы развития нейронного машинного перевода в контексте концепции открытого образования. // Ученые записки университета имени П.Ф. Лесгафта. – 2020. – № 4 (182). – С. 225 - 231.</li> 
<li>Маценов Д.А. Искусственные нейронные сети в переводе. // Лингвистика и перевод: сб. науч. статей / отв. ред. И.М. Нетунаева, А.М. Поликарпов; Сев. (Арктич.) федер. ун-т им. М.В. Ломоносова. – Архангельск: САФУ, 2018. – Вып. 7. – С. 164 - 169.</li>
<li>Мифтахова Р.Г. Машинный перевод. Нейроперевод. / Мифтахова Р.Г., Морозкина Е.А. // Вестник Башкирского университета. - 2019. - Т. 24. №2. – С. 497 - 502.</li>
<li><a href="https://sysblok.ru/knowhow/kak-rabotajut-transformery-krutejshie-nejroseti-nashih-dnej/">Системный Блокъ. Как работают трансформеры — крутейшие нейросети наших дней</a> </li>
</ol>

</body>

</html>